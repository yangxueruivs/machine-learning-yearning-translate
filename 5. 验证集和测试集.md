## 5 验证集和测试集

让我们回到最开始的例子：用户在你的手机应用上上传各种各样的图片，你希望能够自动辨别出哪些是有猫的。

团队从不同的网站上下载了包括有猫的和没猫的图片构成了一个很大的数据集，然后把这个数据三七分为了训练和测试集。利用这些数据，他们构建了一个猫检测器，并且在这两个数据集上效果都不错。

但是当你把这个部署到手机应用上之后，发现这个检测器表现非常差！

![](https://i.imgur.com/D9d1eg3.png)

发生了什么！

你发现这些用户上传的图片跟你的训练集采用的那些网络图片不太一样：用户都是使用手机上传图片，往往会导致低分辨率，模糊，暗光等问题。因为训练和测试集都是网络图片，所以你的算法并不能泛化到实际关心的分布上：手机照片。

在当前的大数据时代之前，训练集合数据集的划分标准一般是70%/30%。这个划分是有效的，但是当你的训练集（在我们的例子中是网络图片）分布和你实际关心的分布（手机照片）不一致时——越来越多的应用有这个问题——这会成为一个很差的方法。

通常定义：

- **训练集** —— 你的算法学习的数据；
- **验证集** —— 用来调参、特征选择和做其他算法调整，有时候也叫做hold-out交叉验证集；
- **测试集** —— 用来评估你的算法性能，但是不能用来作为调参或者算法优化。

一旦你定义好了验证集和测试集，你的团队就会尝试很多的方案，比如找出不同的算法参数中哪个最好。验证集和测试机允许你的团队快速的评估算法的效果。

也就是说，**验证集和测试集的目的在于引导你的团队往对算法系统最重要的方向改进。**

所以，你应该：

	选择能够反映你希望应用的场景的数据来作为验证集和测试集。

因此，你的测试集不应该简单的选择30%的可用数据，也别是你期望的应用场景（手机照片）和你的训练集（网络图片）天然不一致的时候。

如果你还没有发布这个手机app，那就没有任何用户，这样可能也不能得到精确反映你想在未来做好的部分。但你可以尽力逼近这一点。举个例子，让你的朋友们拿着手机拍很多猫的照片然后传给你。一旦这个app发布之后，你就可以使用实际的用户数据来更新你的验证/测试集。

如果你真的没有任何办法去收集可以逼近真实情况的数据，那也许你可以从网络图片开始。但同时也要意识到，这个模型可能没办法泛化。

在验证集和测试集上花多少时间是需要考虑，但不要假设你的训练分布和测试分布一致。尽量挑选能够反映最终模型应用场景的样本，而不是无论什么训练数据都行。

## 5 Your development and test sets

Let’s return to our earlier cat pictures example: You run a mobile app, and users are uploading pictures of many different things to your app. You want to automatically find the cat pictures.

Your team gets a large training set by downloading pictures of cats (positive examples) and non-cats (negative examples) off of different websites. They split the dataset 70%/30% into training and test sets. Using this data, they build a cat detector that works well on the training and test sets.

But when you deploy this classifier into the mobile app, you find that the performance is really poor!

![](https://i.imgur.com/D9d1eg3.png)

What happened?

You figure out that the pictures users are uploading have a different look than the website images that make up your training set: Users are uploading pictures taken with mobile phones, which tend to be lower resolution, blurrier, and poorly lit. Since your training/test sets were made of website images, your algorithm did not generalize well to the actual distribution you care about: mobile phone pictures.

Before the modern era of big data, it was a common rule in machine learning to use a random 70%/30% split to form your training and test sets. This practice can work, but it’s a bad idea in more and more applications where the training distribution (website images in our example above) is different from the distribution you ultimately care about (mobile phone images).

We usually define:

- **Training set**​ — Which you run your learning algorithm on.

- **Dev (development) set**​ — Which you use to tune parameters, select features, and make other decisions regarding the learning algorithm. Sometimes also called the **hold-out cross validation set​**.

- **Test set​** — which you use to evaluate the performance of the algorithm, but not to make any decisions regarding what learning algorithm or parameters to use.

Once you define a dev set (development set) and test set, your team will try a lot of ideas, such as different learning algorithm parameters, to see what works best. The dev and test sets allow your team to quickly see how well your algorithm is doing.

In other words, **​the purpose of the dev and test sets are to direct your team toward the most important changes to make to the machine learning system​.**

So, you should do the following:

	Choose dev and test sets to reflect data you expect to get in the future and want to do well on.

In other words, your test set should not simply be 30% of the available data, especially if you expect your future data (mobile phone images) to be different in nature from your training set (website images).

If you have not yet launched your mobile app, you might not have any users yet, and thus might not be able to get data that accurately reflects what you have to do well on in the future. But you might still try to approximate this. For example, ask your friends to take mobile phone pictures of cats and send them to you. Once your app is launched, you can update your dev/test sets using actual user data.

If you really don’t have any way of getting data that approximates what you expect to get in the future, perhaps you can start by using website images. But you should be aware of the risk of this leading to a system that doesn’t generalize well.

It requires judgment to decide how much to invest in developing great dev and test sets. But don’t assume your training distribution is the same as your test distribution. Try to pick test examples that reflect what you ultimately want to perform well on, rather than whatever data you happen to have for training.
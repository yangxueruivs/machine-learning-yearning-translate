## 8 建立用于优化的单一评估指标 

分类准确率是单一评估指标的一个例子：你在验证集（或测试集）上的分类器返回了一个它正确分类了多少样本的数字。 根据这个指标，如果分类器A的准确率为97％，并且分类器B的准确率为90％，则我们判断分类器A是更好。

相比之下，Precision和Recall*不是单一评估指标：它为评估你的分类器提供了两个数字。 拥有多个评估指标会使比较算法变得更加困难。 假设您的算法结果如下：

![](https://i.imgur.com/3YY37lj.png)

在这里，两个分类器都没有明显的优势，所以它不会为你马上提供更好的选择。

![](https://i.imgur.com/IkuVM2r.png)

在开发过程中，您的团队将尝试很多关于算法架构，模型参数，特征选择等的想法。拥有单一评估指标（如准确率）允许您根据其在此指标上的性能对所有模型进行排序 ，并迅速决定什么是最好的。

如果你真的想同时考虑Precision和Recall，那么我建议你用一种指标把他们结合起来。比如求这两者的平均。或者，你可以计算“F1分数”，这是一种优化之后的求平均的方式，比简单的取平均效果更好**。

![](https://i.imgur.com/MG8N9XT.png)

当你需要在一大堆分类器中作出选择时，单一的评估指标能够大幅提升决策速度。这能给出一个非常清晰的模型优劣排序，以及一个清晰的进展方向。

再举一个例子，假设你要在四个不同的市场分别跟踪你的猫猫分类器：(i)美国(ii)中国(iii)印度(iv)其他。那么就会有四种指标，然后对这它们求平均或加权平均，你就会得到单一的指标。这样的方式是缩减指标最常用的方法之一。

\________________________________________________

\* 猫分类器的Precision是开发（或测试）集中被标记为真正是猫的猫的百分比。 Recall是开发（或测试）集中所有猫图像中正确标记为猫的百分比。 在高Precision和高Recall之间经常需要权衡。

\** 如果您想了解有关F1分数的更多信息，请访问https://en.wikipedia.org/wiki/F1_score。 它是Precision和Recall之间的“调和平均值”，计算为2 /（（1 / Precision）+（1 / Recall））。

## 8 Establish a single-number evaluation metric for your team to optimize

Classification accuracy is an example of a ​single-number evaluation metric​: You run your classifier on the dev set (or test set), and get back a single number about what fraction of examples it classified correctly. According to this metric, if classifier A obtains 97% accuracy, and classifier B obtains 90% accuracy, then we judge classifier A to be superior.

In contrast, Precision and Recall* is not a single-number evaluation metric: It gives two numbers for assessing your classifier. Having multiple-number evaluation metrics makes it harder to compare algorithms. Suppose your algorithms perform as follows:

![](https://i.imgur.com/3YY37lj.png)

Here, neither classifier is obviously superior, so it doesn’t immediately guide you toward picking one.

![](https://i.imgur.com/IkuVM2r.png)

During development, your team will try a lot of ideas about algorithm  architecture, model parameters, choice of features, etc. Having a ​single-number evaluation metric​ such as accuracy allows you to sort all your models according to their performance on this metric, and quickly decide what is working best.

If you really care about both Precision and Recall, I recommend using one of the standard ways to combine them into a single number. For example, one could take the average of precision and recall, to end up with a single number. Alternatively, you can compute the “F1 score,” which is a modified way of computing their average, and works better than simply taking the mean**.

![](https://i.imgur.com/MG8N9XT.png)

Having a single-number evaluation metric speeds up your ability to make a decision when you are selecting among a large number of classifiers. It gives a clear preference ranking among all of them, and therefore a clear direction for progress.

As a final example, suppose you are separately tracking the accuracy of your cat classifier in four key markets: (i) US, (ii) China, (iii) India, and (iv) Other. This gives four metrics. By taking an average or weighted average of these four numbers, you end up with a single number metric. Taking an average or weighted average is one of the most common ways to combine multiple metrics into one.

\________________________________________________

\* The Precision of a cat classifier is the fraction of images in the dev (or test) set it labeled as cats that really are cats. Its Recall is the percentage of all cat images in the dev (or test) set that it correctly labeled as a cat. There is often a tradeoff between having high precision and high recall.

\** If you want to learn more about the F1 score, see ​https://en.wikipedia.org/wiki/F1_score​. It is the “harmonic mean” between Precision and Recall, and is calculated as 2/((1/Precision)+(1/Recall)).
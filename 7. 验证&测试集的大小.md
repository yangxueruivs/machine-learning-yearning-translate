## 7 验证/测试集的大小？

验证集需要足够大，来检测你正在尝试的算法之间的差异。例如，如果分类器A的accuracy为90.0％，分类器B的accuracy为90.1％，那么100个样本的验证集将无法检测到这0.1％的差异。与其他机器学习问题相比，100个样本的验证集很小。常见测试集从1,000到10,000个样本都有。如果有10,000个样本，你就很可能检测到0.1％的改进。

对于成熟和重要的应用程序 - 例如广告，网络搜索和产品推荐 - 我也看到团队有很强的动力去改善哪怕0.01％，因为它对公司的利润有直接影响。在这种情况下，开发设置可能远大于10,000，以便检测更小的改进。

那么测试集的大小呢？它应该足够大，以便对系统的整体性能有高置信度。一种流行的启发式方法是将30％的数据用于测试集。当你有一些适当数量的样本时，这很有用 - 例如100到10,000个样本。但是在大数据时代，我们现在有机器学习问题，有时超过十亿个样本，分配给验证/测试集的数据的比例一直在缩小，即使验证/测试集中的例子的绝对数量在增长。除了评估算法性能所需的验证/测试集之外，不需要过多的验证/测试集。

## 7 How large do the dev/test sets need to be?

The dev set should be large enough to detect differences between algorithms that you are trying out. For example, if classifier A has an accuracy of 90.0% and classifier B has an accuracy of 90.1%, then a dev set of 100 examples would not be able to detect this 0.1% difference. Compared to other machine learning problems I’ve seen, a 100 example dev set is small. Dev sets with sizes from 1,000 to 10,000 examples are common. With 10,000 examples, you will have a good chance of detecting an improvement of 0.1%.

For mature and important applications—for example, advertising, web search, and product recommendations—I have also seen teams that are highly motivated to eke out even a 0.01% improvement, since it has a direct impact on the company’s profits. In this case, the dev set could be much larger than 10,000, in order to detect even smaller improvements. 

How about the size of the test set? It should be large enough to give high confidence in the overall performance of your system. One popular heuristic had been to use 30% of your data for your test set. This works well when you have a modest number of examples—say 100 to 10,000 examples. But in the era of big data where we now have machine learning problems with sometimes more than a billion examples, the fraction of data allocated to dev/test sets has been shrinking, even as the absolute number of examples in the dev/test sets has been growing. There is no need to have excessively large dev/test sets beyond what is needed to evaluate the performance of your algorithms.
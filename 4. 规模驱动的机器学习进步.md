## 4 规模驱动的机器学习进步 Scale drives machine learning progress

很多深度学习（神经网络）的概念几十年前就有了，为什么最近才显示出效果？

导致近期进展的两个最大的原因是：

- 数据获取。人们现在花费在数字设备（笔记本、移动设备）上的时间更多了。这些在数字设备上的行为会生成海量的数据供我们的算法使用；
- 计算性能。我们从最近几年才能够使用这些海量数据去训练神经网络。

具体来说，当我们积累了更多的数据，传统的学习算法比如逻辑回归，往往会达到一个“高地”。这意味着学习曲线会变平，更多的数据也不会提升算法结果:

![](https://i.imgur.com/m2FxabW.png)

似乎传统算法没办法处理这么大量的数据。

如果你对相同的监督学习任务训练一个较小的神经网络(NN)，可能会得到更好的结果:

![](https://i.imgur.com/xMWSlpo.png)

这里的“小神经网络”是指只有几个隐藏层/单元/参数的网络。如果你训练更大的网络，那么最后的结果也会更好[1]：

![](https://i.imgur.com/67uJh0W.png)

所以，当你(i)训练一个很大的网络(ii)拥有海量的数据时，你能得到最佳的结果。

很多其他的细节比如网络架构也很重要，这个领域的创新也很多。但最有效的提升算法性能的方法还是(i)训练更大的网络(ii)获取更多数据。

要做到这两点其实非常困难，本书会在最后讨论具体细节。我们会从对传统方法和神经网络都通用的策略开始，逐渐构建最先进的深度学习系统。

[1] 这个图显示NN在小数据集上会表现得更好。但这种优势没有在大数据量的时候明显。在小数据集领域，对于不同的特征工程，传统算法可能有提升，也可能没有。举个例子，如果你有20条训练样本，用逻辑回归或者神经网络可能没有太大区别，特征工程对结果的影响更大。但如果有1百万条样本，神经网络多半是更好的。

## 4 Scale drives machine learning progress

Many of the ideas of deep learning (neural networks) have been around for decades. Why are these ideas taking off now?

Two of the biggest drivers of recent progress have been:

- Data availability.​ People are now spending more time on digital devices (laptops, mobile devices). Their digital activities generate huge amounts of data that we can feed to our learning algorithms.
- Computational scale. ​We started just a few years ago to be able to train neural networks that are big enough to take advantage of the huge datasets we now have.

In detail, even as you accumulate more data, usually the performance of older learning algorithms, such as logistic regression, “plateaus.” This means its learning curve “flattens out,” and the algorithm stops improving even as you give it more data:

![](https://i.imgur.com/m2FxabW.png)

It was as if the older algorithms didn’t know what to do with all the data we now have.

If you train a small neutral network (NN) on the same supervised learning task, you might get slightly better performance:

![](https://i.imgur.com/xMWSlpo.png)

Here, by “Small NN” we mean a neural network with only a small number of hidden units/layers/parameters. Finally, if you train larger and larger neural networks, you can obtain even better performance[1]:

![](https://i.imgur.com/67uJh0W.png)

Thus, you obtain the best performance when you (i) Train a very large neural network, so that you are on the green curve above; (ii) Have a huge amount of data.

Many other details such as neural network architecture are also important, and there has been much innovation here. But one of the more reliable ways to improve an algorithm’s performance today is still to (i) train a bigger  network and (ii) get more data.

The process of how to accomplish (i) and (ii) are surprisingly complex. This book will discuss the details at length. We will start with general strategies that are useful for both traditional learning algorithms and neural networks, and build up to the most modern strategies for building deep learning systems.

[1]:This diagram shows NNs doing better in the regime of small datasets. This effect is less consistent than the effect of NNs doing well in the regime of huge datasets. In the small data regime, depending on how the features are hand-engineered, traditional algorithms may or may not do better. For example, if you have 20 training  examples, it might not matter much whether you use logistic regression or a neural network; the hand-engineering of features will have a bigger effect than the choice of algorithm. But if you have 1 million examples, I would favor the neural network.
## 6 验证集和测试集需要满足相同的分布

基于你最大的市场，你把猫图片数据分成了四个区域：(i)美国，(ii)中国，(iii)印度，(iv)其他。为了构造验证和测试集，假设我们把美国和印度作为验证集，中国和其他作为测试集。换句话说，我们可以随机的进行划分，抽取其中两个作为验证集，剩下的作为测试集，这样对吗？

![](https://i.imgur.com/nSkONLl.png)

一旦验证和测试集被定义好，你的团队就可以开始专注于提高在验证集上的表现。因此，验证集需要反映你最想提升的部分场景：在四个市场都表现的好，而不是其中的两个。

另外一个问题是不同的验证/测试集分布：有可能你的团队发现模型在验证集上很好，但只在测试集上表现不好。有很多无用功会花在这上边，尽量避免这种情况。

举个例子，你的团队开发了一个在验证集上运行良好但在测试集上不好的系统。如果验证和测试集满足同一个分布，那么你可以非常确定出现了什么问题：在验证集上过你喝了。 显然，解决方案是获取更多的验证集数据。

但如果这两个数据集不满足相同分布，那么你要考虑的就很多了。比如：

1. 验证集过拟合；
2. 测试集比验证集更难。所以你的算法可能不会在上边表现的那么好，更大的进展也不太可能了；
3. 测试集也没那么难，仅仅是和验证集不一样。所以验证集上的优化手段可能不起作用。这种情况下，对验证集的优化可能是一种浪费；

研究机器学习应用程序已经很困难了。不匹配的验证和测试集额外引入了验证集的改善是否影响测试集性能的不确定性。由于验证和测试集不匹配，很难弄清楚什么是有效的，从而使得工作的优先选择变得更加困难。

如果你正在做第三方benchmark测试，他们的创建者可能已指定了不同分布的开发人员和测试集。与开发和测试集来自同一分布的情况相比，运气，而非技能，会对你在此类benchmark测试中的性能产生更大的影响。开发在一个分布上训练并且能泛化到其他数据上的学习算法是一个重要的研究课题。但是，如果你的目标是在特定的机器学习应用程序上取得进展而不是进行研究，我建议你尝试选择从同一分布中提取开发和测试集。这将使你的团队更高效。

## 6 Your dev and test sets should come from the same distribution

![](https://i.imgur.com/nSkONLl.png)

You have your cat app image data segmented into four regions, based on your largest markets: (i) US, (ii) China, (iii) India, and (iv) Other. To come up with a dev set and a test set, say we put US and India in the dev set; China and Other in the test set. In other words, we can randomly assign two of these segments to the dev set, and the other two to the test set, right?

Once you define the dev and test sets, your team will be focused on improving dev set performance. Thus, the dev set should reflect the task you want to improve on the most: To do well on all four geographies, and not only two.

There is a second problem with having different dev and test set distributions: There is a chance that your team will build something that works well on the dev set, only to find that it does poorly on the test set. I’ve seen this result in much frustration and wasted effort. Avoid letting this happen to you.

As an example, suppose your team develops a system that works well on the dev set but not the test set. If your dev and test sets had come from the same distribution, then you would have a very clear diagnosis of what went wrong: You have overfit the dev set. The obvious cure is to get more dev set data.

But if the dev and test sets come from different distributions, then your options are less clear. Several things could have gone wrong:

1. You had overfit to the dev set.

2. The test set is harder than the dev set. So your algorithm might be doing as well as could be expected, and no further significant improvement is possible.

3. The test set is not necessarily harder, but just different, from the dev set. So what works well on the dev set just does not work well on the test set. In this case, a lot of your work to improve dev set performance might be wasted effort.

Working on machine learning applications is hard enough. Having mismatched dev and test sets introduces additional uncertainty about whether improving on the dev set distribution also improves test set performance. Having  mismatched dev and test sets makes it harder to figure out what is and isn’t working, and thus makes it harder to prioritize what to work on. 

If you are working on a 3rd party benchmark problem, their creator might have specified dev and test sets that come from different distributions. Luck, rather than skill, will have a greater impact on your performance on such benchmarks compared to if the dev and test sets come from the same distribution. It is an important research problem to develop learning algorithms that are trained on one distribution and generalize well to another. But if your goal is to make progress on a specific machine learning application rather than make research progress, I recommend trying to choose dev and test sets that are drawn from the same distribution. This will make your team more efficient.
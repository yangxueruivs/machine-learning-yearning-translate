## 11 何时修改验证/测试集和评价指标

在开始新项目时，我尝试快速选择开发/测试集，因为这为团队提供了明确定义的目标。

我通常会要求我的团队在不到一周的时间内提出初始验证/测试集和初始指标。最好提出一些不完美的东西并快速前进，而不是过度思考。但这一周的时间表不适用于成熟的应用程序。例如，反垃圾邮件是一种成熟的深度学习应用程序。我已经看到在已经成熟的系统上工作的团队花费数月时间来获得更好的验证/测试集。

如果您后来意识到您的初始验证/测试集或度量标准弄错了，则无论如何都要快速更改它们。例如，如果您的验证设置+指标将分类器A排在分类器B之上，但您的团队认为分类器B实际上对您的产品更优越，那么这可能表示您需要更改您的验证/测试集或评估指标。

验证集/评价指标错误评级分类器A有三个可能的主要原因：

<u>1. 实际应用数据的分布与验证/测试集不同。</u>

假设您的初始验证/测试集主要是成年猫的图片。您运行猫应用，并发现用户正在上传比预期更多的小猫图像。因此，验证/测试集分布不能代表您需要做好的实际分布。在这种情况下，请更新您的验证/测试集以使其更具代表性。

![](https://i.imgur.com/N5SZk69.png)

<u>2. 验证集过拟合</u>

在验证集上重复评估想法的过程会导致您的算法逐渐“过拟合”到验证集。完成开发后，您将在测试集上评估您的系统。如果您发现您的验证集性能远远优于您的测试集性能，则表明您已经过度设置了验证集。在这种情况下，需要选取一个新的验证集。

如果您需要跟踪团队的进度，还可以定期评估您的系统——例如每周一次或每月一次——在测试集上。但是，不要使用测试集做出有关算法的任何决定，包括是否回滚到前一周的系统。如果你这样做，你将开始适应测试集，并且不再指望它对你的系统性能进行完全无偏的估计（如果你发表研究论文，或者你可能需要使用这个指标做出重要的商业决策）。

<u>3.该指标衡量的不是项目需要优化的内容。</u>

假设对于您的猫应用，您的度量标准是分类准确性。该度量当前将分类器A排序为优于分类器B.但是假设您尝试了两种算法，并且发现分类器A允许偶尔的色情图像滑过。尽管分类器A更准确，偶尔的色情图片留下的坏印象意味着它的表现是不可接受的。你会怎么做？

这里，度量标准无法确定算法B实际上比您的产品的算法A更好的事实。因此，您无法再信任度量标准来选择最佳算法。是时候改变评估指标了。例如，您可以更改指标，严重惩罚色情图片的出现。我强烈建议选择一个新指标并使用新指标明确定义团队的新目标，而不是在没有可信指标的情况下继续进行太长时间，并且还要在分类器之间手动选择。

在项目期间更改验证/测试集或评估指标是很常见的。拥有初始验证/测试集和度量标准可帮助您快速迭代。如果您发现验证/测试集或指标不再指向您的团队正确的方向，那没什么大不了的！只需更改它们，确保您的团队了解新方向。

## 11 When to change dev/test sets and metrics

When starting out on a new project, I try to quickly choose dev/test sets, since this gives the team a well-defined target to aim for.

I typically ask my teams to come up with an initial dev/test set and an initial metric in less than one week—rarely longer. It is better to come up with something imperfect and get going quickly, rather than overthink this. But this one week timeline does not apply to mature applications. For example, anti-spam is a mature deep learning application. I have seen teams working on already-mature systems spend months to acquire even better dev/test sets.

If you later realize that your initial dev/test set or metric missed the mark, by all means change them quickly. For example, if your dev set + metric ranks classifier A above classifier B, but your team thinks that classifier B is actually superior for your product, then this might be a sign that you need to change your dev/test sets or your evaluation metric.

There are three main possible causes of the dev set/metric incorrectly rating classifier A higher:

<u>1. The actual distribution you need to do well on is different from the dev/test sets.</u>

Suppose your initial dev/test set had mainly pictures of adult cats. You ship your cat app, and find that users are uploading a lot more kitten images than expected. So, the dev/test set distribution is not representative of the actual distribution you need to do well on. In this case, update your dev/test sets to be more representative.

![](https://i.imgur.com/N5SZk69.png)

<u>2. You have overfit to the dev set.</u>

The process of repeatedly evaluating ideas on the dev set causes your algorithm to gradually “overfit” to the dev set. When you are done developing, you will evaluate your system on the test set. If you find that your dev set performance is much better than your test set performance, it is a sign that you have overfit to the dev set. In this case, get a fresh dev set.

If you need to track your team’s progress, you can also evaluate your system regularly—say once per week or once per month—on the test set. But do not use the test set to make any decisions regarding the algorithm, including whether to roll back to the previous week’s system. If you do so, you will start to overfit to the test set, and can no longer count on it to give a completely unbiased estimate of your system’s performance (which you would need if you’re publishing research papers, or perhaps using this metric to make important business decisions).

<u>3. The metric is measuring something other than what the project needs to optimize.</u>

Suppose that for your cat application, your metric is classification accuracy. This metric currently ranks classifier A as superior to classifier B. But suppose you try out both algorithms, and find classifier A is allowing occasional pornographic images to slip through. Even though classifier A is more accurate, the bad impression left by the occasional pornographic image means its performance is unacceptable. What do you do?

Here, the metric is failing to identify the fact that Algorithm B is in fact better than Algorithm A for your product. So, you can no longer trust the metric to pick the best algorithm. It is time to change evaluation metrics. For example, you can change the metric to heavily penalize letting through pornographic images. I would strongly recommend picking a new metric and using the new metric to explicitly define a new goal for the team, rather than proceeding for too long without a trusted metric and reverting to manually choosing among classifiers.

It is quite common to change dev/test sets or evaluation metrics during a project. Having an initial dev/test set and metric helps you iterate quickly. If you ever find that the dev/test sets or metric are no longer pointing your team in the right direction, it’s not a big deal! Just change them and make sure your team knows about the new direction.